{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test the chatbot models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "import csv\n",
    "# read in raw data\n",
    "with open('clean_withid_noblanks_part2.csv') as csvfile:\n",
    "    readCSV = csv.reader(csvfile, delimiter=',')\n",
    "    raw_all = []\n",
    "    match_all = []\n",
    "    time_all = []\n",
    "    slot_all = []\n",
    "    for row in readCSV:\n",
    "        match_all.append(row[0])\n",
    "        time_all.append(row[1])\n",
    "        slot_all.append(row[2])\n",
    "        raw_all.append(row[3])\n",
    "print(len(raw_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train model with subset and test the model against another subset\n",
    "Ntrain = 10000\n",
    "raw = raw_all[:Ntrain]\n",
    "tt = time_all[:Ntrain]\n",
    "slot = slot_all[:Ntrain]\n",
    "match = match_all[:Ntrain]\n",
    "\n",
    "raw_test = raw_all[Ntrain+1:Ntrain+10000]\n",
    "tt_test = time_all[Ntrain+1:Ntrain+10000]\n",
    "slot_test = slot_all[Ntrain+1:Ntrain+10000]\n",
    "match_test = match_all[Ntrain+1:Ntrain+10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TF-IDF approach\n",
    "import nltk\n",
    "import numpy as np\n",
    "import string # to process standard python strings\n",
    "sent_tokens = raw\n",
    "lemmer = nltk.stem.WordNetLemmatizer()\n",
    "#WordNet is a semantically-oriented dictionary of English included in NLTK.\n",
    "def LemTokens(tokens):\n",
    "    return [lemmer.lemmatize(token) for token in tokens]\n",
    "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
    "def LemNormalize(text):\n",
    "    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "def response(user_response):\n",
    "    robo_response=''\n",
    "    sent_tokens.append(user_response)\n",
    "    TfidfVec = TfidfVectorizer(tokenizer=LemNormalize, stop_words='english')\n",
    "    tfidf = TfidfVec.fit_transform(sent_tokens)\n",
    "    vals = cosine_similarity(tfidf[-1], tfidf)\n",
    "    idx=vals.argsort()[0][-2]\n",
    "    flat = vals.flatten()\n",
    "    flat.sort()\n",
    "    req_tfidf = flat[-2]\n",
    "    if(req_tfidf==0):\n",
    "        robo_response=robo_response+''\n",
    "        return robo_response\n",
    "    else:\n",
    "        robo_response = robo_response+sent_tokens[idx]\n",
    "        return robo_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chatterbot\n",
    "from chatterbot import ChatBot\n",
    "chatbot = ChatBot('dotai', read_only=True) # trained in chatterbot.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the model\n",
    "Nrec = len(raw_test)\n",
    "rec = 0\n",
    "dt = 10.\n",
    "response1 = []\n",
    "response2 = []\n",
    "response0 = []\n",
    "while rec < Nrec-1:\n",
    "    if match_test[rec+1] == match_test[rec] and float(tt_test[rec+1]) < float(tt_test[rec])+dt:\n",
    "        print(['Found conversation at rec: ',rec])\n",
    "        user_response = raw_test[rec]\n",
    "        response0.append(raw_test[rec+1])\n",
    "        response1.append(response(user_response))\n",
    "        sent_tokens.remove(user_response)\n",
    "        response2.append(chatbot.get_response(user_response))\n",
    "        rec = rec + 2\n",
    "    else:\n",
    "        rec = rec + 1\n",
    "\n",
    "# save responses in txt files\n",
    "with open('response0.txt', 'w') as f:\n",
    "    for item in response0:\n",
    "        f.write(\"%s\\n\" % item)\n",
    "with open('response1.txt', 'w') as f:\n",
    "    for item in response1:\n",
    "        f.write(\"%s\\n\" % item)\n",
    "with open('response2.txt', 'w') as f:\n",
    "    for item in response2:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load saved data\n",
    "with open('response0.txt') as f:\n",
    "    response0 = f.read().splitlines()\n",
    "with open('response1.txt') as f:\n",
    "    response1 = f.read().splitlines()\n",
    "with open('response2.txt') as f:\n",
    "    response2 = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check how many responses from TF-IDF appoach are valid\n",
    "Nr = 0\n",
    "for i in response1:\n",
    "    if i:\n",
    "        Nr = Nr+1\n",
    "print(str(Nr)+' valid response out of '+str(len(response1))+' for similarity model')\n",
    "print((len(response1)-Nr)/len(response1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word count\n",
    "word_tmp = [nltk.word_tokenize(i.lower()) for i in response0]\n",
    "word0 = [item for sublist in word_tmp for item in sublist]\n",
    "word_tmp = [nltk.word_tokenize(i.lower()) for i in response1]\n",
    "word1 = [item for sublist in word_tmp for item in sublist]\n",
    "word_tmp = [nltk.word_tokenize(i.lower()) for i in response2]\n",
    "word2 = [item for sublist in word_tmp for item in sublist]\n",
    "\n",
    "from collections import Counter\n",
    "C0 = Counter(word0)\n",
    "C1 = Counter(word1)\n",
    "C2 = Counter(word2)\n",
    "word_sort = []\n",
    "frac0_sort = []\n",
    "frac1_sort = []\n",
    "frac2_sort = []\n",
    "for k,v in C0.most_common()[:5]:\n",
    "    word_sort.append(k)\n",
    "    frac0_sort.append(v/len(word0)*100)\n",
    "    frac1_sort.append(C1[k]/len(word1)*100)\n",
    "    frac2_sort.append(C2[k]/len(word2)*100)\n",
    "print(C0.most_common()[:5])\n",
    "print(C1.most_common()[:5])\n",
    "print(C2.most_common()[:5])\n",
    "\n",
    "w = csv.writer(open(\"count0.csv\", \"w\"))\n",
    "for key, val in C0.most_common():\n",
    "    w.writerow([key, val])\n",
    "w = csv.writer(open(\"count1.csv\", \"w\"))\n",
    "for key, val in C1.most_common():\n",
    "    w.writerow([key, val])\n",
    "w = csv.writer(open(\"count2.csv\", \"w\"))\n",
    "for key, val in C2.most_common():\n",
    "    w.writerow([key, val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot word frequecies as bar plot\n",
    "import matplotlib.pyplot as plt \n",
    "name_list = word_sort\n",
    "x =list(range(len(word_sort)))\n",
    "total_width, n = 0.8, 3\n",
    "width = total_width / n\n",
    " \n",
    "plt.bar(x, frac0_sort, width=width, label='data',fc = 'y')\n",
    "for i in range(len(x)):\n",
    "    x[i] = x[i] + width\n",
    "plt.bar(x, frac1_sort, width=width, label='similarity model',tick_label = name_list,fc = 'r')\n",
    "for i in range(len(x)):\n",
    "    x[i] = x[i] + width\n",
    "plt.bar(x, frac2_sort, width=width, label='chatterbot',tick_label = name_list,fc = 'b')\n",
    "plt.ylabel('frequency (%)')\n",
    "plt.xlabel('word')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.savefig('top5_frequency.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
